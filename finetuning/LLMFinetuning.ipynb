{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2eSvM9zX_2d3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import torch\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.26\" trl peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2v_X2fA0Df5"
      },
      "source": [
        "* We support Llama, Mistral, CodeLlama, TinyLlama, Vicuna, Open Hermes etc\n",
        "* And Yi, Qwen ([llamafied](https://huggingface.co/models?sort=trending&search=qwen+llama)), Deepseek, all Llama, Mistral derived archs.\n",
        "* We support 16bit LoRA or 4bit QLoRA. Both 2x faster.\n",
        "* `max_seq_length` can be set to anything, since we do automatic RoPE Scaling via [kaiokendev's](https://kaiokendev.github.io/til) method.\n",
        "* With [PR 26037](https://github.com/huggingface/transformers/pull/26037), we support downloading 4bit models **4x faster**! [Our repo](https://huggingface.co/unsloth) has Llama, Mistral 4bit models.\n",
        "* [**NEW**] We make Gemma 6 trillion tokens **2.5x faster**! See our [Gemma notebook](https://colab.research.google.com/drive/10NbwlsRChbma1v55m8LAPYG15uQv6HLo?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_ygUyjOFQqS"
      },
      "outputs": [],
      "source": [
        "#To help handle dependency issues on colab\n",
        "# !python -m pip install --upgrade pip\n",
        "# !pip install -q xformers<0.0.19 triton==2.0.0 -U\n",
        "# !pip install --upgrade torch torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unsloth code borrowed and modified from guide: https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing#scrollTo=8LKIA_qnVKOz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "7460bdb5-d675-48c7-aa11-1afcc01e4eab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth: Fast Llama patching release 2024.4\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.2.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. Xformers = 0.0.25.post1. FA = False.\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: unsloth/tinyllama-bnb-4bit can only handle sequence lengths of at most 2048.\n",
            "But with kaiokendev's RoPE scaling of 4.0, it can be magically be extended to 8192!\n",
            "Unused kwargs: ['quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 8192 \n",
        "dtype = torch.float16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/mistral-7b-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
        "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
        "    \"unsloth/llama-2-13b-bnb-4bit\",\n",
        "    \"unsloth/codellama-34b-bnb-4bit\",\n",
        "    \"unsloth/tinyllama-bnb-4bit\",\n",
        "    \"unsloth/gemma-7b-bnb-4bit\", \n",
        "    \"unsloth/gemma-2b-bnb-4bit\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/tinyllama-bnb-4bit\", # \"unsloth/tinyllama\" for 16bit loading\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3xvsMEWyJbZ"
      },
      "source": [
        "**[NOTE]** TinyLlama's internal maximum sequence length is 2048. We use RoPE Scaling to extend it to 4096 with Unsloth!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!\n",
        "\n",
        "**[NOTE]** We set `gradient_checkpointing=False` ONLY for TinyLlama since Unsloth saves tonnes of memory usage. This does NOT work for `llama-2-7b` or `mistral-7b` since the memory usage will still exceed Tesla T4's 15GB. GC recomputes the forward pass during the backward pass, saving loads of memory.\n",
        "\n",
        "`**[IF YOU GET OUT OF MEMORY]**` set `gradient_checkpointing` to `True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bZsfBuZDeCL",
        "outputId": "bc054690-19ca-4ab7-c712-6837e04478bb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2024.4 patched 22 layers with 22 QKV layers, 22 O layers and 22 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 4, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0, # Currently only supports dropout = 0\n",
        "    bias = \"none\",    # Currently only supports bias = \"none\"\n",
        "    use_gradient_checkpointing = True, # @@@ IF YOU GET OUT OF MEMORY - set to True @@@\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep\n",
        "We now use the Alpaca dataset from [yahma](https://huggingface.co/datasets/yahma/alpaca-cleaned), which is a filtered version of 52K of the original [Alpaca dataset](https://crfm.stanford.edu/2023/03/13/alpaca.html). You can replace this code section with your own data prep.\n",
        "\n",
        "**[NOTE]** To train only on completions (ignoring the user's input) read TRL's docs [here](https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only).\n",
        "\n",
        "**[NOTE]** Remember to add the **EOS_TOKEN** to the tokenized output!! Otherwise you'll get infinite generations!\n",
        "\n",
        "If you want to use the `ChatML` template for ShareGPT datasets, try our conversational [notebook](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing).\n",
        "\n",
        "For text completions like novel writing, try this [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data handling code borrowed from ToolLlama repo: https://github.com/OpenBMB/ToolBench"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RTmIzz8x_K9s"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import transformers\n",
        "from enum import auto, Enum\n",
        "from typing import List, Any, Dict\n",
        "from transformers.trainer_pt_utils import LabelSmoother\n",
        "import dataclasses\n",
        "\n",
        "\n",
        "IGNORE_TOKEN_ID = LabelSmoother.ignore_index\n",
        "\n",
        "class SupervisedDataset(Dataset):\n",
        "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
        "\n",
        "    def __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer, template=\"tool-llama\"):\n",
        "        super(SupervisedDataset, self).__init__()\n",
        "\n",
        "        print(\"Formatting inputs...\")\n",
        "        sources = [example[\"conversations\"] for example in raw_data]\n",
        "        self.template = template\n",
        "        data_dict = preprocess(sources, tokenizer, self.template)\n",
        "        self.input_ids = data_dict[\"input_ids\"]\n",
        "        self.labels = data_dict[\"labels\"]\n",
        "        self.attention_mask = data_dict[\"attention_mask\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return dict(\n",
        "            input_ids=self.input_ids[i],\n",
        "            labels=self.labels[i],\n",
        "            attention_mask=self.attention_mask[i],\n",
        "        )\n",
        "class LazySupervisedDataset(Dataset):\n",
        "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
        "\n",
        "    def __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer, template=\"tool-llama\"):\n",
        "        super(LazySupervisedDataset, self).__init__()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.tokenizer = tokenizer\n",
        "        self.raw_data = raw_data\n",
        "        self.cached_data_dict = {}\n",
        "        self.template = template\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.raw_data)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        if i in self.cached_data_dict:\n",
        "            return self.cached_data_dict[i]\n",
        "\n",
        "        ret = preprocess([self.raw_data[i][\"conversations\"]], self.tokenizer, self.template)\n",
        "        ret = dict(\n",
        "            input_ids=ret[\"input_ids\"][0],\n",
        "            labels=ret[\"labels\"][0],\n",
        "            attention_mask=ret[\"attention_mask\"][0],\n",
        "        )\n",
        "        self.cached_data_dict[i] = ret\n",
        "\n",
        "        return ret\n",
        "\n",
        "def preprocess(\n",
        "    sources,\n",
        "    tokenizer: transformers.PreTrainedTokenizer,\n",
        "    template: str=\"tool-llama-single-round\"):\n",
        "\n",
        "    conv = Conversation(\n",
        "    name=\"tool-llama-single-round\",\n",
        "    system=\"\", # We put the system message in the specific SFT data. Remember to use the same system message in inference.\n",
        "    roles=(\"System\", \"User\", \"Function\", \"Assistant\"),\n",
        "    messages=(),\n",
        "    offset=0,\n",
        "    sep_style=SeparatorStyle.ONLY_LAST_ASSISTANT,\n",
        "    sep=\"\\n\",\n",
        "    sep2=\"</s>\")\n",
        "\n",
        "    if template == \"tool-llama\":\n",
        "      roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\n",
        "    elif template == \"tool-llama-single-round\" or template == \"tool-llama-multi-rounds\":\n",
        "      roles = {\"system\": conv.roles[0], \"user\": conv.roles[1], \"function\": conv.roles[2], \"assistant\": conv.roles[3]}\n",
        "\n",
        "    # Apply prompt templates\n",
        "    conversations = []\n",
        "    for i, source in enumerate(sources):\n",
        "        conv.messages = []\n",
        "        for j, sentence in enumerate(source):\n",
        "            role = roles[sentence[\"from\"]]\n",
        "            conv.append_message(role, sentence[\"value\"])\n",
        "        conversations.append(conv.get_prompt())\n",
        "\n",
        "    # Tokenize conversations\n",
        "    input_ids = tokenizer(\n",
        "        conversations,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=\"max_length\",\n",
        "        max_length=tokenizer.model_max_length,\n",
        "        truncation=True,\n",
        "    ).input_ids\n",
        "    targets = input_ids.clone()\n",
        "\n",
        "    # Mask targets. Only compute loss on the assistant outputs.\n",
        "    sep = conv.sep + conv.roles[-1] + \": \"\n",
        "    for conversation, target in zip(conversations, targets):\n",
        "        total_len = int(target.ne(tokenizer.pad_token_id).sum())\n",
        "        turns = conversation.split(conv.sep2)\n",
        "        cur_len = 1\n",
        "        target[:cur_len] = IGNORE_TOKEN_ID\n",
        "        for i, turn in enumerate(turns):\n",
        "            if turn == \"\":\n",
        "                continue\n",
        "            turn_len = len(tokenizer(turn).input_ids)\n",
        "\n",
        "            parts = turn.split(sep)\n",
        "\n",
        "            # only train on the last assistant reply, treat the history chat as instruction\n",
        "            prefix = parts[:-1]\n",
        "            instruction = \"\"\n",
        "            for part in prefix:\n",
        "                instruction += part\n",
        "                instruction += sep\n",
        "\n",
        "            # \"-2\" is hardcoded for the LLaMA tokenizer to make the offset correct.\n",
        "            instruction_len = len(tokenizer(instruction).input_ids) - 2\n",
        "\n",
        "            # Ignore the user instructions\n",
        "            target[cur_len : cur_len + instruction_len] = IGNORE_TOKEN_ID\n",
        "            cur_len += turn_len\n",
        "\n",
        "        target[cur_len:] = IGNORE_TOKEN_ID\n",
        "\n",
        "        if False:  # Inspect and check the correctness of masking\n",
        "            z = target.clone()\n",
        "            z = torch.where(z == IGNORE_TOKEN_ID, tokenizer.unk_token_id, z)\n",
        "            rank0_print(tokenizer.decode(z))\n",
        "\n",
        "        if cur_len < tokenizer.model_max_length:\n",
        "            if cur_len != total_len:\n",
        "                target[:] = IGNORE_TOKEN_ID\n",
        "                print(\n",
        "                    f\"WARNING: tokenization mismatch: {cur_len} vs. {total_len}.\"\n",
        "                    f\" (ignored)\"\n",
        "                )\n",
        "    return dict(\n",
        "        input_ids=input_ids,\n",
        "        labels=targets,\n",
        "        attention_mask=input_ids.ne(tokenizer.pad_token_id),\n",
        "    )\n",
        "\n",
        "class SeparatorStyle(Enum):\n",
        "    \"\"\"Separator styles.\"\"\"\n",
        "\n",
        "    ADD_COLON_SINGLE = auto()\n",
        "    ADD_COLON_TWO = auto()\n",
        "    ADD_COLON_SPACE_SINGLE = auto()\n",
        "    NO_COLON_SINGLE = auto()\n",
        "    ADD_NEW_LINE_SINGLE = auto()\n",
        "    DOLLY = auto()\n",
        "    RWKV = auto()\n",
        "    PHOENIX = auto()\n",
        "    ONLY_LAST_ASSISTANT = auto()\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class Conversation:\n",
        "    \"\"\"A class that keeps all conversation history.\"\"\"\n",
        "\n",
        "    # The name of this template\n",
        "    name: str\n",
        "    # The System prompt\n",
        "    system: str\n",
        "    # Two roles\n",
        "    roles: List[str]\n",
        "    # All messages\n",
        "    messages: List[List[str]]\n",
        "    # Offset of few shot examples\n",
        "    offset: int\n",
        "    # Separators\n",
        "    sep_style: SeparatorStyle\n",
        "    sep: str\n",
        "    sep2: str = None\n",
        "    # Stop criteria (the default one is EOS token)\n",
        "    stop_str: str = None\n",
        "    # Stops generation if meeting any token in this list\n",
        "    stop_token_ids: List[int] = None\n",
        "\n",
        "    def get_prompt(self) -> str:\n",
        "        \"\"\"Get the prompt for generation.\"\"\"\n",
        "        if self.sep_style == SeparatorStyle.ONLY_LAST_ASSISTANT:\n",
        "            seps = [self.sep, self.sep2]\n",
        "            ret = \"\"\n",
        "            for i, (role, message) in enumerate(self.messages):\n",
        "                if i + 1 == len(self.messages) and message:\n",
        "                    ret += role + \": \" + str(message) + seps[1]\n",
        "                elif message:\n",
        "                    ret += role + \": \" + str(message) + seps[0]\n",
        "                else:\n",
        "                    ret += role + \":\"\n",
        "            return ret\n",
        "        elif self.sep_style == SeparatorStyle.ADD_COLON_SINGLE:\n",
        "            ret = self.system + self.sep\n",
        "            for role, message in self.messages:\n",
        "                if message:\n",
        "                    ret += role + \": \" + message + self.sep\n",
        "                else:\n",
        "                    ret += role + \":\"\n",
        "            return ret\n",
        "        elif self.sep_style == SeparatorStyle.ADD_COLON_TWO:\n",
        "            seps = [self.sep, self.sep2]\n",
        "            ret = self.system + seps[0]\n",
        "            for i, (role, message) in enumerate(self.messages):\n",
        "                try:\n",
        "                    if message:\n",
        "                        ret += role + \": \" + message + seps[i % 2]\n",
        "                    else:\n",
        "                        ret += role + \":\"\n",
        "                except:\n",
        "                    continue\n",
        "            return ret\n",
        "        elif self.sep_style == SeparatorStyle.ADD_COLON_SPACE_SINGLE:\n",
        "            ret = self.system + self.sep\n",
        "            for role, message in self.messages:\n",
        "                if message:\n",
        "                    ret += role + \": \" + message + self.sep\n",
        "                else:\n",
        "                    ret += role + \": \"  # must be end with a space\n",
        "            return ret\n",
        "        elif self.sep_style == SeparatorStyle.NO_COLON_SINGLE:\n",
        "            ret = self.system\n",
        "            for role, message in self.messages:\n",
        "                if message:\n",
        "                    ret += role + message + self.sep\n",
        "                else:\n",
        "                    ret += role\n",
        "            return ret\n",
        "        elif self.sep_style == SeparatorStyle.ADD_NEW_LINE_SINGLE:\n",
        "            ret = self.system + self.sep\n",
        "            for role, message in self.messages:\n",
        "                if message:\n",
        "                    ret += role + \"\\n\" + message + self.sep\n",
        "                else:\n",
        "                    ret += role + \"\\n\"\n",
        "            return ret\n",
        "        elif self.sep_style == SeparatorStyle.DOLLY:\n",
        "            seps = [self.sep, self.sep2]\n",
        "            ret = self.system\n",
        "            for i, (role, message) in enumerate(self.messages):\n",
        "                if message:\n",
        "                    ret += role + \":\\n\" + message + seps[i % 2]\n",
        "                    if i % 2 == 1:\n",
        "                        ret += \"\\n\\n\"\n",
        "                else:\n",
        "                    ret += role + \":\\n\"\n",
        "            return ret\n",
        "        elif self.sep_style == SeparatorStyle.RWKV:\n",
        "            ret = self.system\n",
        "            for i, (role, message) in enumerate(self.messages):\n",
        "                if message:\n",
        "                    ret += (\n",
        "                        role\n",
        "                        + \": \"\n",
        "                        + message.replace(\"\\r\\n\", \"\\n\").replace(\"\\n\\n\", \"\\n\")\n",
        "                    )\n",
        "                    ret += \"\\n\\n\"\n",
        "                else:\n",
        "                    ret += role + \":\"\n",
        "            return ret\n",
        "        elif self.sep_style == SeparatorStyle.PHOENIX:\n",
        "            ret = self.system\n",
        "            for role, message in self.messages:\n",
        "                if message:\n",
        "                    ret += role + \": \" + \"<s>\" + message + \"</s>\"\n",
        "                else:\n",
        "                    ret += role + \": \" + \"<s>\"\n",
        "            return ret\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid style: {self.sep_style}\")\n",
        "\n",
        "    def append_message(self, role: str, message: str):\n",
        "        \"\"\"Append a new message.\"\"\"\n",
        "        self.messages.append([role, message])\n",
        "\n",
        "    def to_gradio_chatbot(self):\n",
        "        \"\"\"Convert the history to gradio chatbot format\"\"\"\n",
        "        ret = []\n",
        "        for i, (role, msg) in enumerate(self.messages[self.offset :]):\n",
        "            if i % 2 == 0:\n",
        "                ret.append([msg, None])\n",
        "            else:\n",
        "                ret[-1][-1] = msg\n",
        "        return ret\n",
        "\n",
        "    def to_openai_api_messages(self):\n",
        "        \"\"\"Convert the conversation to OpenAI chat completion format.\"\"\"\n",
        "        ret = [{\"role\": \"system\", \"content\": self.system}]\n",
        "\n",
        "        for i, (_, msg) in enumerate(self.messages[self.offset :]):\n",
        "            if i % 2 == 0:\n",
        "                ret.append({\"role\": \"user\", \"content\": msg})\n",
        "            else:\n",
        "                if msg is not None:\n",
        "                    ret.append({\"role\": \"assistant\", \"content\": msg})\n",
        "        return ret\n",
        "\n",
        "    def copy(self):\n",
        "        return Conversation(\n",
        "            name=self.name,\n",
        "            system=self.system,\n",
        "            roles=self.roles,\n",
        "            messages=[[x, y] for x, y in self.messages],\n",
        "            offset=self.offset,\n",
        "            sep_style=self.sep_style,\n",
        "            sep=self.sep,\n",
        "            sep2=self.sep2,\n",
        "            stop_str=self.stop_str,\n",
        "            stop_token_ids=self.stop_token_ids,\n",
        "        )\n",
        "\n",
        "    def dict(self):\n",
        "        return {\n",
        "            \"name\": self.name,\n",
        "            \"system\": self.system,\n",
        "            \"roles\": self.roles,\n",
        "            \"messages\": self.messages,\n",
        "            \"offset\": self.offset,\n",
        "        }\n",
        "\n",
        "conv_template = Conversation(\n",
        "    name=\"tool-llama-single-round\",\n",
        "    system=\"\", # We put the system message in the specific SFT data. Remember to use the same system message in inference.\n",
        "    roles=(\"System\", \"User\", \"Function\", \"Assistant\"),\n",
        "    messages=(),\n",
        "    offset=0,\n",
        "    sep_style=SeparatorStyle.ONLY_LAST_ASSISTANT,\n",
        "    sep=\"\\n\",\n",
        "    sep2=\"</s>\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QF-v4V7Y-6gu",
        "outputId": "c523cb71-ae4c-425c-95b1-e3f87dbe2da7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "#train 746, #eval 16\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n",
        "import json\n",
        "import numpy as np\n",
        "# dataset_cls = (\n",
        "#     LazySupervisedDataset if data_args.lazy_preprocess else SupervisedDataset\n",
        "# )\n",
        "dataset_cls = LazySupervisedDataset\n",
        "template_name = 'tool-llama-single-round'\n",
        "print(\"Loading data...\")\n",
        "raw_data = json.load(open('toolllama_G123_dfs_eval.json', \"r\"))\n",
        "# if data_args.eval_data_path is not None:\n",
        "#     train_raw_data = raw_data\n",
        "#     eval_raw_data = json.load(open(data_args.eval_data_path, \"r\"))\n",
        "if 1 == 0:\n",
        "  print('dummy if')\n",
        "else:\n",
        "    # Split train/test\n",
        "    perm = np.random.permutation(len(raw_data))\n",
        "    split = int(len(perm) * 0.98)\n",
        "    train_indices = perm[:split]\n",
        "    eval_indices = perm[split:]\n",
        "    train_raw_data = [raw_data[i] for i in train_indices]\n",
        "    eval_raw_data = [raw_data[i] for i in eval_indices]\n",
        "print(f\"#train {len(train_raw_data)}, #eval {len(eval_raw_data)}\")\n",
        "train_dataset = dataset_cls(train_raw_data, tokenizer=tokenizer, template=template_name)\n",
        "eval_dataset = dataset_cls(eval_raw_data, tokenizer=tokenizer, template=template_name)\n",
        "data_module = dict(train_dataset=train_dataset, eval_dataset=eval_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fvTgiymW6_F9"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, Optional\n",
        "\n",
        "@dataclass\n",
        "class TrainingArguments(transformers.TrainingArguments):\n",
        "    cache_dir: Optional[str] = field(default=None)\n",
        "    optim: str = field(default=\"adamw_torch\")\n",
        "    source_model_max_length: int = field(\n",
        "        default=2048,\n",
        "        metadata={\n",
        "            \"help\": \"Original maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n",
        "        },\n",
        "    )\n",
        "    model_max_length: int = field(\n",
        "        default=8192,\n",
        "        metadata={\n",
        "            \"help\": \"Expanded maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n",
        "        },\n",
        "    )\n",
        "# training_args = TrainingArguments(output_dir = '/', fp16 = not torch.cuda.is_bf16_supported(), bf16 = torch.cuda.is_bf16_supported())\n",
        "training_args = TrainingArguments(output_dir = '/', fp16 = True, bf16 = False, per_device_train_batch_size=2, per_device_eval_batch_size=8, num_train_epochs=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fBfGns2k3QUp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
        "ddp = world_size != 1\n",
        "device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)} if ddp else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "6prPO-823UCn",
        "outputId": "c3f1ea1b-304c-426a-fb79-2d207e55be81"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 746 | Num Epochs = 1\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 1\n",
            "\\        /    Total batch size = 2 | Total steps = 373\n",
            " \"-____-\"     Number of trainable parameters = 3,153,920\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='373' max='373' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [373/373 1:07:36, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "output_dir = '/'\n",
        "\n",
        "trainer = Trainer(model=model, tokenizer=tokenizer, args=training_args, **data_module)\n",
        "trainer.train()\n",
        "trainer.save_state()\n",
        "# safe_save_model_for_hf_trainer(trainer=trainer, output_dir=output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "iztuND0InRyy"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(output_dir='./trained_model/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR3gIAX-SM2q",
        "outputId": "90359c2f-b17b-420c-8317-460a41a8d139"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<s>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nYou are AutoGPT, you can use many tools(functions) to do the following task.\\nFirst I will give you the task description, and your task start.\\nAt each step, you need to give your thought to analyze the status now and what to do next, with a function call to actually excute your step. Your output should follow this format:\\nThought:\\nAction\\nAction Input:\\n\\nAfter the call, you will get the call result, and you are now in a new state.\\nThen you will analyze your status now, then decide what to do next...\\nAfter many (Thought-call) pairs, you finally perform the task, then you can give your finial answer.\\nRemember: \\n1.the state change is irreversible, you can\\'t go back to one of the former state, if you want to restart the task, say \"I give up and restart\".\\n2.All the thought is short, at most in 5 sentence.\\n3.You can do more then one trys, so if your plan is to continusly try some conditions, you can do one of the conditions per try.\\nLet\\'s Begin!\\nTask description: You should use functions to help handle the real time user querys. Remember:\\n1.ALWAYS call \"Finish\" function at the end of the task. And the final answer should contain enough information to show to the user,If you can\\'t handle the task, or you find that function calls always fail(the function is not valid now), use function Finish->give_up_and_restart.\\n2.Do not use origin tool names, use only subfunctions\\' names.\\nYou have access of the following tools:\\n1.comparable_companies: Our ML algorithm finds the most similar firms to any company. Find 10x more companies 10x faster!The API will return up to 50 similar companies for any company domain that you provide. Data points provided for every company include: Name, # of employees, Revenue in USD, Revenue Growth, Description, Logo image URL, HQ Location, Country, and Linkedin URL.Use this API to power your web application with \"related companies\" sections, or use it to leverage internal databases with new data points.\\n\\nSpecifically, you have access to the following APIs: [{\\'name\\': \\'logo_for_comparable_companies\\', \\'description\\': \\'This is the subfunction for tool \"comparable_companies\", you can use this tool.The description of this function is: \"The Logo API is used to show company logos in the frontend. The API link is already included in results returned by the Similar companies API.\"\\', \\'parameters\\': {\\'type\\': \\'object\\', \\'properties\\': {\\'domain\\': {\\'type\\': \\'string\\', \\'description\\': \\'(Required) The company domain for which you want to retrieve a logo for\\', \\'example_value\\': \\'unilever.com\\'}}, \\'required\\': [\\'domain\\'], \\'optional\\': []}}, {\\'name\\': \\'similar_for_comparable_companies\\', \\'description\\': \\'This is the subfunction for tool \"comparable_companies\", you can use this tool.The description of this function is: \"The key feature of the comparable-companies API. This endpoint will return a list of companies that are similar to the one that has been provided. The endpoint will also return information about the queried domain through the variable \"frontend_company_inf\"\\', \\'parameters\\': {\\'type\\': \\'object\\', \\'properties\\': {\\'domain\\': {\\'type\\': \\'string\\', \\'description\\': \\'(Required) The company domain for which you want to look up similar companies for (e.g. unilever.com). Providing a URL works as well, we will then parse the domain on our end.\\', \\'example_value\\': \\'unilever.com\\'}, \\'limit\\': {\\'type\\': \\'integer\\', \\'description\\': \\'(Optional) Max number of results to return. A lower limit leads to faster response times. Results beyond that limit are included in the response field \\\\\\\\\"next_companies\\\\\\\\\" which only shows their domain name and similarity percentage.\\', \\'example_value\\': 50}}, \\'required\\': [\\'domain\\', \\'limit\\'], \\'optional\\': []}}, {\\'name\\': \\'rating_for_comparable_companies\\', \\'description\\': \\'This is the subfunction for tool \"comparable_companies\", you can use this tool.The description of this function is: \"Enables the user to rate the output returned by the /similar API. It is advisable to refresh results after ratings have been submitted as the algorithm fine-tunes its output based on them. Re-searching a company is always free of charge.\\\\n\\\\n*Example*: If a u\"\\', \\'parameters\\': {\\'type\\': \\'object\\', \\'properties\\': {\\'comp_company_id\\': {\\'type\\': \\'string\\', \\'description\\': \\'(Required) domain_name of the comparable company which is being rated\\', \\'example_value\\': \\'nestle.com\\'}, \\'rating\\': {\\'type\\': \\'integer\\', \\'description\\': \\'(Required) The rating given by the user ranging from 0 to 10. Ratings above 5 will fine-tune the algorithm to find more companies similar to that one.\\', \\'example_value\\': 10}, \\'main_company_id\\': {\\'type\\': \\'string\\', \\'description\\': \\'(Required) The company domain which was queried\\', \\'example_value\\': \\'unilever.com\\'}}, \\'required\\': [\\'comp_company_id\\', \\'rating\\', \\'main_company_id\\'], \\'optional\\': []}}, {\\'name\\': \\'contacts_for_comparable_companies\\', \\'description\\': \\'This is the subfunction for tool \"comparable_companies\", you can use this tool.The description of this function is: \"Returns a list of contact persons (incl. email addresses) at a given company.\\\\n\\\\nQuerying the same domain multiple times will only cost a single search credit.\\\\n\\\\nQuerying a domain that you have already queried through the /similar endpoint will not cost any c\"\\', \\'parameters\\': {\\'type\\': \\'object\\', \\'properties\\': {\\'domain\\': {\\'type\\': \\'string\\', \\'description\\': \\'(Required) The company domain for which you want to look up contacts for (e.g. unilever.com). Providing a URL works as well, we will then parse the domain on our end.\\', \\'example_value\\': \\'unilever.com\\'}}, \\'required\\': [\\'domain\\'], \\'optional\\': []}}, {\\'name\\': \\'Finish\\', \\'description\\': \\'If you believe that you have obtained a result that can answer the task, please call this function to provide the final answer. Alternatively, if you recognize that you are unable to proceed with the task in the current state, call this function to restart. Remember: you must ALWAYS call this function at the end of your attempt, and the only part that will be shown to the user is the final answer, so it should contain sufficient information.\\', \\'parameters\\': {\\'type\\': \\'object\\', \\'properties\\': {\\'return_type\\': {\\'type\\': \\'string\\', \\'enum\\': [\\'give_answer\\', \\'give_up_and_restart\\']}, \\'final_answer\\': {\\'type\\': \\'string\\', \\'description\\': \\'The final answer you want to give the user. You should have this field if \"return_type\"==\"give_answer\"\\'}}, \\'required\\': [\\'return_type\\']}}]\\n\\n### Input:\\n\\nI am organizing a conference and would like to invite companies similar to \\'ibm.com\\'. Can you retrieve the list of 50 similar companies along with their names, number of employees, revenue in USD, revenue growth, description, logo image URL, HQ location, country, and Linkedin URL? Including the logo of each company would be highly appreciated.\\nBegin!\\n\\n\\n### Response:\\n\\n\\nThought: I am calling the \"similar_for_comparable_companies\" function with the argument \"domain\" set to \"ibm.com\". This function will retrieve the list of 50 similar companies along with their names, number of employees, revenue in USD, revenue growth, description, logo URL, HQ location, country, and Linkedin URL. By including the logo of each company, I can provide the user with a list of similar companies that are relevant to the conference.\\nAction: similar_for_comparable_companies\\nAction Input: {\\n  \"domain\": \"ibm.com\"\\n}</s>']"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tinyllama_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    tinyllama_prompt.format(\n",
        "        \"You are AutoGPT, you can use many tools(functions) to do the following task.\\nFirst I will give you the task description, and your task start.\\nAt each step, you need to give your thought to analyze the status now and what to do next, with a function call to actually excute your step. Your output should follow this format:\\nThought:\\nAction\\nAction Input:\\n\\nAfter the call, you will get the call result, and you are now in a new state.\\nThen you will analyze your status now, then decide what to do next...\\nAfter many (Thought-call) pairs, you finally perform the task, then you can give your finial answer.\\nRemember: \\n1.the state change is irreversible, you can't go back to one of the former state, if you want to restart the task, say \\\"I give up and restart\\\".\\n2.All the thought is short, at most in 5 sentence.\\n3.You can do more then one trys, so if your plan is to continusly try some conditions, you can do one of the conditions per try.\\nLet's Begin!\\nTask description: You should use functions to help handle the real time user querys. Remember:\\n1.ALWAYS call \\\"Finish\\\" function at the end of the task. And the final answer should contain enough information to show to the user,If you can't handle the task, or you find that function calls always fail(the function is not valid now), use function Finish->give_up_and_restart.\\n2.Do not use origin tool names, use only subfunctions' names.\\nYou have access of the following tools:\\n1.comparable_companies: Our ML algorithm finds the most similar firms to any company. Find 10x more companies 10x faster!The API will return up to 50 similar companies for any company domain that you provide. Data points provided for every company include: Name, # of employees, Revenue in USD, Revenue Growth, Description, Logo image URL, HQ Location, Country, and Linkedin URL.Use this API to power your web application with \\\"related companies\\\" sections, or use it to leverage internal databases with new data points.\\n\\nSpecifically, you have access to the following APIs: [{'name': 'logo_for_comparable_companies', 'description': 'This is the subfunction for tool \\\"comparable_companies\\\", you can use this tool.The description of this function is: \\\"The Logo API is used to show company logos in the frontend. The API link is already included in results returned by the Similar companies API.\\\"', 'parameters': {'type': 'object', 'properties': {'domain': {'type': 'string', 'description': '(Required) The company domain for which you want to retrieve a logo for', 'example_value': 'unilever.com'}}, 'required': ['domain'], 'optional': []}}, {'name': 'similar_for_comparable_companies', 'description': 'This is the subfunction for tool \\\"comparable_companies\\\", you can use this tool.The description of this function is: \\\"The key feature of the comparable-companies API. This endpoint will return a list of companies that are similar to the one that has been provided. The endpoint will also return information about the queried domain through the variable \\\"frontend_company_inf\\\"', 'parameters': {'type': 'object', 'properties': {'domain': {'type': 'string', 'description': '(Required) The company domain for which you want to look up similar companies for (e.g. unilever.com). Providing a URL works as well, we will then parse the domain on our end.', 'example_value': 'unilever.com'}, 'limit': {'type': 'integer', 'description': '(Optional) Max number of results to return. A lower limit leads to faster response times. Results beyond that limit are included in the response field \\\\\\\\\\\"next_companies\\\\\\\\\\\" which only shows their domain name and similarity percentage.', 'example_value': 50}}, 'required': ['domain', 'limit'], 'optional': []}}, {'name': 'rating_for_comparable_companies', 'description': 'This is the subfunction for tool \\\"comparable_companies\\\", you can use this tool.The description of this function is: \\\"Enables the user to rate the output returned by the /similar API. It is advisable to refresh results after ratings have been submitted as the algorithm fine-tunes its output based on them. Re-searching a company is always free of charge.\\\\n\\\\n*Example*: If a u\\\"', 'parameters': {'type': 'object', 'properties': {'comp_company_id': {'type': 'string', 'description': '(Required) domain_name of the comparable company which is being rated', 'example_value': 'nestle.com'}, 'rating': {'type': 'integer', 'description': '(Required) The rating given by the user ranging from 0 to 10. Ratings above 5 will fine-tune the algorithm to find more companies similar to that one.', 'example_value': 10}, 'main_company_id': {'type': 'string', 'description': '(Required) The company domain which was queried', 'example_value': 'unilever.com'}}, 'required': ['comp_company_id', 'rating', 'main_company_id'], 'optional': []}}, {'name': 'contacts_for_comparable_companies', 'description': 'This is the subfunction for tool \\\"comparable_companies\\\", you can use this tool.The description of this function is: \\\"Returns a list of contact persons (incl. email addresses) at a given company.\\\\n\\\\nQuerying the same domain multiple times will only cost a single search credit.\\\\n\\\\nQuerying a domain that you have already queried through the /similar endpoint will not cost any c\\\"', 'parameters': {'type': 'object', 'properties': {'domain': {'type': 'string', 'description': '(Required) The company domain for which you want to look up contacts for (e.g. unilever.com). Providing a URL works as well, we will then parse the domain on our end.', 'example_value': 'unilever.com'}}, 'required': ['domain'], 'optional': []}}, {'name': 'Finish', 'description': 'If you believe that you have obtained a result that can answer the task, please call this function to provide the final answer. Alternatively, if you recognize that you are unable to proceed with the task in the current state, call this function to restart. Remember: you must ALWAYS call this function at the end of your attempt, and the only part that will be shown to the user is the final answer, so it should contain sufficient information.', 'parameters': {'type': 'object', 'properties': {'return_type': {'type': 'string', 'enum': ['give_answer', 'give_up_and_restart']}, 'final_answer': {'type': 'string', 'description': 'The final answer you want to give the user. You should have this field if \\\"return_type\\\"==\\\"give_answer\\\"'}}, 'required': ['return_type']}}]\", # instruction\n",
        "        \"\\nI am organizing a conference and would like to invite companies similar to 'ibm.com'. Can you retrieve the list of 50 similar companies along with their names, number of employees, revenue in USD, revenue growth, description, logo image URL, HQ location, country, and Linkedin URL? Including the logo of each company would be highly appreciated.\\nBegin!\\n\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 512, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ejIt2xSNKKp",
        "outputId": "b58a5395-e0e3-43eb-b5dc-fd656d571a3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU = Tesla T4. Max memory = 14.748 GB.\n",
            "0.879 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"./trained_model/lora_model/\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CgqR2B0vmCt"
      },
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yle1gGB3vmWK"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"lora_model\", \n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"What is a famous tall tower in Paris?\", \n",
        "        \"\", \n",
        "        \"\", \n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhc9u6HAvr3b"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_TmxAoavvYW"
      },
      "outputs": [],
      "source": [
        "# Save to q5_k_m GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q5_k_m\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0a88fd16d72442198803c4c082bdd5ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_444156136b864dd4896c9f83135772f6",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0da54be740b943c38a4aa805692b35f2",
            "value": 1
          }
        },
        "0da54be740b943c38a4aa805692b35f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "22a3640c1e344b5c922e488a9a2feaca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e5a6d4cd4bb45879ed67ab7d9cbcc16": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "444156136b864dd4896c9f83135772f6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "47b91d628c824007ba2a899908543fc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_544c9ffdfd6346d19865bf34d43e26a3",
              "IPY_MODEL_0a88fd16d72442198803c4c082bdd5ab",
              "IPY_MODEL_e245c09fe14a49929a8274fdc356fd25"
            ],
            "layout": "IPY_MODEL_5cb07c2ef3be4a9f90cac8a53565040f"
          }
        },
        "544c9ffdfd6346d19865bf34d43e26a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93cfec82c4e24cd8bbf6d0044c92f9a4",
            "placeholder": "​",
            "style": "IPY_MODEL_7aea5b2b4cb944efae3dc1de39b72f83",
            "value": "Generating train split: "
          }
        },
        "5cb07c2ef3be4a9f90cac8a53565040f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7aea5b2b4cb944efae3dc1de39b72f83": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "93cfec82c4e24cd8bbf6d0044c92f9a4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e245c09fe14a49929a8274fdc356fd25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e5a6d4cd4bb45879ed67ab7d9cbcc16",
            "placeholder": "​",
            "style": "IPY_MODEL_22a3640c1e344b5c922e488a9a2feaca",
            "value": " 3000/0 [00:46&lt;00:00, 120.63 examples/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
